# -*- coding: utf-8 -*-
"""Data Extraction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oCsSGunCqjHowMTU0Hxgc3JHqQG7wD3_

### Hi! So far we have collected and represented the data in forms that can be easily trained. All the data are provided in weekly frequency except Weather feature, which only provides monthly update. As the dataset is relatively small for sequential network architecture, weather data is not combined in the dataset at first. If we find it necessary, we will include weather feature as part of the input. The current feature list is ['Date', 'Imports', 'Exports', 'Net Imports', 'Gasoline Prices', 'percent_change']; after discarding NaN data, there are approximately 1600 datapoints.
"""

!pip install netcdf4 pydap

import gdown
import numpy as np
import pandas as pd
import xarray as xr
import xarray as xr

exports_url = "https://drive.google.com/file/d/1J27j-yAZhe9tiQiJbctpL44QJ97nA4sV/view?usp=drive_link"
imports_url = "https://drive.google.com/file/d/1HS8YdIXwkHTxnq0CzJzJX1wUi_dvtznM/view?usp=drive_link"
net_imports_url = "https://drive.google.com/file/d/1ibxyKMBfs1WFNuYyUkmxf9k9jC_zuniX/view?usp=drive_link"
gas_prices_url = "https://drive.google.com/file/d/1rODElv0oLcJGLZufTfoLFhastfiMaLfX/view?usp=sharing"
tavg_url = "https://drive.google.com/file/d/1l6RLFWmVHtsQP4nGPKta9zvJAiJR71DS/view?usp=sharing"

exports = pd.read_csv('https://drive.google.com/uc?export=download&id=' + exports_url.split('/')[-2], skiprows=2)
imports = pd.read_csv('https://drive.google.com/uc?export=download&id=' + imports_url.split('/')[-2], skiprows=2)
net_imports = pd.read_csv('https://drive.google.com/uc?export=download&id=' + net_imports_url.split('/')[-2], skiprows=2)
gas_prices = pd.read_csv('https://drive.google.com/uc?export=download&id=' + gas_prices_url.split('/')[-2], skiprows=2)

# gas_prices.shape
gas_prices['percent_change'] = gas_prices['Weekly U.S. All Grades All Formulations Retail Gasoline Prices  (Dollars per Gallon)'].pct_change() * 100
gas_prices.dropna(inplace=True)
gas_prices['percent_change'] = gas_prices['percent_change'].map("{:.2f}%".format)
gas_prices.head()

#imports.head()

net_imports = net_imports.iloc[:, :2]
net_imports.head()

exports = exports.iloc[:, :2]
exports.head()

imports.head()

############################ ALL FEATURES EXCEPT WEATHER ARE ORGANIZED IN merge_data ############################

gas_prices['Date'] = pd.to_datetime(gas_prices['Date'], format='%b %d, %Y')
net_imports['Date'] = pd.to_datetime(net_imports['Date'], format='%b %d, %Y')
imports['Date'] = pd.to_datetime(imports['Date'], format='%b %d, %Y')
exports['Date'] = pd.to_datetime(exports['Date'], format='%b %d, %Y')

net_imports['Date'] = net_imports['Date'] - pd.to_timedelta(net_imports['Date'].dt.weekday, unit='D')
imports['Date'] = imports['Date'] - pd.to_timedelta(imports['Date'].dt.weekday, unit='D')
exports['Date'] = exports['Date'] - pd.to_timedelta(exports['Date'].dt.weekday, unit='D')
merged_data = pd.merge(gas_prices, net_imports, on='Date', how='inner')
merged_data = pd.merge(merged_data, imports, on='Date', how='inner')
merged_data = pd.merge(merged_data, exports, on='Date', how='inner')
merged_data.head()


# print(merged_data.columns)  # Verify renaming was successful
# print(merged_data.columns.tolist())
merged_data = merged_data.rename(columns={
    'Weekly U.S. Net Imports of Crude Oil and Petroleum Products  (Thousand Barrels per Day)': 'Net Imports',
    'Weekly U.S. All Grades All Formulations Retail Gasoline Prices  (Dollars per Gallon)': 'Gasoline Prices',
    'Weekly U.S. Imports of Crude Oil and Petroleum Products  (Thousand Barrels per Day)' : 'Imports',
    'Weekly U.S. Exports of Crude Oil and Petroleum Products  (Thousand Barrels per Day)' : 'Exports'
})

merged_data = merged_data[['Date', 'Imports', 'Exports', 'Net Imports', 'Gasoline Prices', 'percent_change']]

print(merged_data.head()['Imports'] - merged_data.head()['Exports'] == merged_data.head()['Net Imports'])
print(merged_data.shape)
print(list(merged_data.columns))
merged_data.head()

merged_data.head()      # This is the complete dataset (without Weather feature) (it provides data in weekly basis)

train_size = int(0.7 * len(merged_data))
val_size = int(0.15 * len(merged_data))
test_size = len(merged_data) - train_size - val_size

train_data = merged_data[ : train_size]
val_data = merged_data[train_size : train_size + val_size]
test_data = merged_data[train_size + val_size : ]

print(f"Training set size: {train_data.shape}")
print(f"Validation set size: {val_data.shape}")
print(f"Testing set size: {test_data.shape}")

debug_size = int(0.1 * train_size)        # Randomly choose 10% consecutive entries from the training dataset as debugging dataset for sanity test
start_idx = np.random.randint(0, train_size - debug_size)
debug_data = train_data[start_idx:start_idx + debug_size]

debug_data.head()

"""# Weather Data

Note: Dataset has avg temp per month for various latitude/longitude points in the continental US. The dataset was initially extracted from here: https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.ncdc%3AC00332.

Initial intuition: More extreme temperatures may lead to higher gasoline prices

## Save the weather data (to_pickle)

Commented out because we only need to do this once
"""

# gdown.download('https://drive.google.com/uc?id=1l6RLFWmVHtsQP4nGPKta9zvJAiJR71DS', 'nclimgrid_tavg.nc', quiet=False)
# db = xr.open_dataset('nclimgrid_tavg.nc', engine="netcdf4")

# # Iterate over each year
# for i in range(32):
#   db_tavg = db.variables['tavg'][:,:,:]
#   db_time = db.variables['time']
#   db_lat = db.variables['lat']
#   db_long = db.variables['lon']

#   start_time = 1176 + i * 12
#   end_time = 1176 + (i + 1) * 12
#   year = 1993 + i
#   db_tavg = db_tavg[start_time:end_time]
#   db_time = db_time[start_time:end_time]

#   df = pd.DataFrame({'Time': [], 'Latitude': [], 'Longitude': [], 'Temp': []})

#   time = db_time.values
#   latitude = db_lat.values
#   longitude = db_long.values

#   time_grid, lat_grid, lon_grid = np.meshgrid(time, latitude, longitude, indexing='ij')

#   time_flat = time_grid.flatten()
#   lat_flat = lat_grid.flatten()
#   lon_flat = lon_grid.flatten()
#   temp_flat = db_tavg.values.flatten()

#   # Create a pandas DataFrame with columns for time, latitude, longitude, and temperature
#   df = pd.DataFrame({
#       'time': time_flat,
#       'latitude': lat_flat,
#       'longitude': lon_flat,
#       'temperature': temp_flat
#   })

#   # Time, lat, lon
#   # 1556, 596, 1385

#   # Drop values that are nan (not valid within the continental US)
#   df = df.dropna()
#   df.to_pickle(f'avg_temp_{year}.pkl')

"""## Get the weather data (read_pickle)

Right now, we split the data into chunks based on year. In the future, we will likely reduce the size of the dataset by focusing only on certain latitudes and longitudes (such as those of major US cities).
"""

# Get these from GDrive
links = 'https://drive.google.com/file/d/1NRsZEY_V4BLAO1dS3ATYAYarhgC1M2S8/view?usp=drive_link, https://drive.google.com/file/d/1--mxpbEOtO6KUms5QchCGn4Nr-esk3Pr/view?usp=drive_link, https://drive.google.com/file/d/1-9XVF6HXPduILEZsiUytnihHZYKYOL9P/view?usp=drive_link, https://drive.google.com/file/d/1--uAFc38dP3b65Mc6X048mJOF0Mida3G/view?usp=drive_link, https://drive.google.com/file/d/1-DXv1fqzAKhycgrtLXwnbfdMhS2bd6hJ/view?usp=drive_link, https://drive.google.com/file/d/1-296vjGCDPp79ECGrCXkGYrp-0Wsu0eF/view?usp=drive_link, https://drive.google.com/file/d/1-KdqjQI0sb8LTKtK1CJip5t2OF7JBrWQ/view?usp=drive_link, https://drive.google.com/file/d/1-C8U2qIW9EzrkUI7om6eniSxgt-SovMr/view?usp=drive_link, https://drive.google.com/file/d/1-Jzo9N_H5xEO-OpZDgIy2pic6kV76NV2/view?usp=drive_link, https://drive.google.com/file/d/1-LdeBU-d9gZloVUTKJmrK1SDS5GPDuFZ/view?usp=drive_link, https://drive.google.com/file/d/1-KFRDB9H8D6K_OfaEgeddiwzUuFlD47K/view?usp=drive_link, https://drive.google.com/file/d/1-QzwUl_2JHMgU-rOL8nO_VpQq-STeffV/view?usp=drive_link, https://drive.google.com/file/d/1-VwyQpYQ6pNlRLcuJOUguggKTcEEl1q2/view?usp=drive_link, https://drive.google.com/file/d/1-Xm52Ygs28TFUDoSfm1CwtGfEQDrxr0i/view?usp=drive_link, https://drive.google.com/file/d/1-W7zxGX8TxQpUX06ZsgnWPYMsYya0xHx/view?usp=drive_link, https://drive.google.com/file/d/1-SWCb_Hd_cP4r9iqIGM31R9wFBBumx_q/view?usp=drive_link, https://drive.google.com/file/d/1-ZuW9QDzfklwENSv3jYB0X9xJEVfMIiO/view?usp=drive_link, https://drive.google.com/file/d/1-_GcHm7BDaCiggos4T12B0NoguA55sUe/view?usp=drive_link, https://drive.google.com/file/d/1-f6hE3cSqZf54Ac414ZafQJgUNDFApsB/view?usp=drive_link, https://drive.google.com/file/d/1-mIMLs48YiSHORu4gjXXI9OVSyxCmhwB/view?usp=drive_link, https://drive.google.com/file/d/1-n6KQagxJJLA38J11uV9vH0nZ3AF5NvW/view?usp=drive_link, https://drive.google.com/file/d/1-oPUOPwhgiMhHJt_2677BfeI0KdCoeRv/view?usp=drive_link, https://drive.google.com/file/d/1-p7issdOZv4bK-5X1nyTr8hQ5bCy8wIE/view?usp=drive_link, https://drive.google.com/file/d/1-rjrY-dS5Co4zrDeX5CiNUkZ1f6nmm5h/view?usp=drive_link, https://drive.google.com/file/d/1-v1l8IbKsMWFa1M73Vr3WNlR_ye_oOKD/view?usp=drive_link, https://drive.google.com/file/d/10-wLcAOkNKECLXBH_vQfs6eHh-2GQ0S8/view?usp=drive_link, https://drive.google.com/file/d/106ZCkzZdDAJocLgvhezanC9anLDralOP/view?usp=drive_link, https://drive.google.com/file/d/10EOXMGE95MHwe0hTk5YVWdBGmANYX7GP/view?usp=drive_link, https://drive.google.com/file/d/108hBksVRFdWoSWDSK9uAabqmjFnZNxTE/view?usp=drive_link, https://drive.google.com/file/d/10B8u88PHZKR9HJQEUOsm4AScXUduBSM4/view?usp=drive_link, https://drive.google.com/file/d/10Cwe4rf15xDQ9GATM7X4IbLZbI3oX8O-/view?usp=drive_link, https://drive.google.com/file/d/10Dt1XwpcdM45S4um8mm75VrZSbIf3AJJ/view?usp=drive_link'
year_id_map = {}
links = links.split(',')
for i in range(len(links)):
  year_id_map[1993 + i] = links[i].split('/')[-2]

def get_tavg_year_df(year):
  file_id = year_id_map[year]
  url = f'https://drive.google.com/uc?id={file_id}'
  output = f'avg_temp_{year}.pkl'
  gdown.download(url, output, quiet=True)
  return pd.read_pickle(output)

tavg_1993_df = get_tavg_year_df(1993)
tavg_1993_df.head()