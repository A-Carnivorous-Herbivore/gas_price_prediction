# -*- coding: utf-8 -*-
"""Copy of Feature Importance.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xsBgCtU6BbDd_hDlUnOnbVdkvIymlB0w

There is a summary at the bottom of this notebook that summarizes the work done in this notebook for this milestone! (Comments are included with code cells along the way as well.)

## Initial Setup
"""

# !pip install netcdf4 pydap

import torch
import gdown
import random
import numpy as np
import pandas as pd
import xarray as xr
import xarray as xr
import datetime as dt
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

"""## Setup Data

*Notes for this can be found in the Data Exploration notebook.*
"""

# set seed value for reproducibility
def set_seed(seed):
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    np.random.seed(seed)
    random.seed(seed)
seed = 42
set_seed(seed)

exports_url = "https://drive.google.com/file/d/1J27j-yAZhe9tiQiJbctpL44QJ97nA4sV/view?usp=drive_link"
imports_url = "https://drive.google.com/file/d/1HS8YdIXwkHTxnq0CzJzJX1wUi_dvtznM/view?usp=drive_link"
net_imports_url = "https://drive.google.com/file/d/1ibxyKMBfs1WFNuYyUkmxf9k9jC_zuniX/view?usp=drive_link"
gas_prices_url = "https://drive.google.com/file/d/1rODElv0oLcJGLZufTfoLFhastfiMaLfX/view?usp=sharing"
tavg_url = "https://drive.google.com/file/d/1l6RLFWmVHtsQP4nGPKta9zvJAiJR71DS/view?usp=sharing"
tavg_region_url = "https://drive.google.com/file/d/1y-XvXUpjxxHyl5q3aOpzZvyWWumFfk1r/view?usp=drive_link"

exports = pd.read_csv('https://drive.google.com/uc?export=download&id=' + exports_url.split('/')[-2], skiprows=2)
imports = pd.read_csv('https://drive.google.com/uc?export=download&id=' + imports_url.split('/')[-2], skiprows=2)
net_imports = pd.read_csv('https://drive.google.com/uc?export=download&id=' + net_imports_url.split('/')[-2], skiprows=2)
gas_prices = pd.read_csv('https://drive.google.com/uc?export=download&id=' + gas_prices_url.split('/')[-2], skiprows=2)
temps = pd.read_pickle('https://drive.google.com/uc?export=download&id=' + tavg_region_url.split('/')[-2])

exports = exports.drop(columns=['Unnamed: 11'])
exports['Date'] = pd.to_datetime(exports['Date'])
imports = imports.drop(columns=['Unnamed: 36'])
imports['Date'] = pd.to_datetime(imports['Date'])
gas_prices['Date'] = pd.to_datetime(gas_prices['Date'])

# Last value of each df is always NaN
exports = exports[:-1]
imports = imports[:-1]
gas_prices = gas_prices[:-1]

export_columns = {}
for column in exports.columns:
  export_columns[column] = ' '.join(column.replace('Weekly U.S.', '').replace('Exports of', '').replace('(Thousand Barrels per Day)', '(Exports)').split())

import_columns = {}
for column in imports.columns:
  import_columns[column] = ' '.join(column.replace('Weekly U.S.', '').replace('Imports of', '').replace('(Thousand Barrels per Day)', '(Imports)').split())

exports = exports.rename(columns = export_columns)
imports = imports.rename(columns = import_columns)
gas_prices = gas_prices.rename(columns = {'Weekly U.S. All Grades All Formulations Retail Gasoline Prices  (Dollars per Gallon)': 'Gas Prices'})

west = temps[temps['region'] == 'West']
midwest = temps[temps['region'] == 'Midwest']
south = temps[temps['region'] == 'South']
northeast = temps[temps['region'] == 'Northeast']
all_regions = [west, midwest, south, northeast]

# Merge
merged_df_all_features = pd.merge_asof(gas_prices, imports, on='Date', direction='backward')
merged_df_all_features = pd.merge_asof(merged_df_all_features, exports, on='Date', direction='backward')

for region in all_regions:
  curr_region = region.iloc[0]['region']
  region = region.add_suffix(f'_{curr_region}')
  merged_df_all_features = pd.merge_asof(merged_df_all_features, region[[f'time_{curr_region}', f'population_weighted_temp_{curr_region}']], direction='backward', left_on='Date', right_on=f'time_{curr_region}')
  merged_df_all_features = merged_df_all_features.drop(columns=[f'time_{curr_region}'])

# Find Net Imports (Similar to net_imports df but with all dates)
reduced_columns = ['Crude Oil', 'Total Petroleum Products', 'Crude Oil and Petroleum Products']
for col in reduced_columns:
  merged_df_all_features[col + ' (Net Imports)'] = merged_df_all_features[col + ' (Imports)'] - merged_df_all_features[col + ' (Exports)']

cols = ['Date', 'Gas Prices'] + [col + ' (Imports)' for col in reduced_columns] + [col + ' (Exports)' for col in reduced_columns] + [col + ' (Net Imports)' for col in reduced_columns] + [f'population_weighted_temp_{region}' for region in ['West', 'Northeast', 'South', 'Midwest']]

merged_df_simple = merged_df_all_features[cols].copy()

for col in merged_df_simple.columns:
  if len(merged_df_simple[merged_df_simple[col].isna()]) > 0:
    print(f"Missing data for {col}:", len(merged_df_simple[merged_df_simple[col].isna()]))

merged_df_simple.loc[:, 'Total Petroleum Products (Exports)'] = merged_df_simple['Total Petroleum Products (Exports)'].ffill()
merged_df_simple.loc[:, 'Total Petroleum Products (Net Imports)'] = merged_df_simple['Total Petroleum Products (Exports)'] - merged_df_simple['Total Petroleum Products (Imports)']

for col in merged_df_simple.columns:
  if col not in ['Date']:
    merged_df_simple[f'{col}_diff_weekly'] = merged_df_simple[col].diff()
    merged_df_simple[f'{col}_diff_monthly'] = merged_df_simple[col] - merged_df_simple[col].shift(4)

"""# Feature Importance"""

import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
import plotly.graph_objects as go
import plotly.express as px

"""## Linear Regression Feature Importance

We use the absolute coefficient for linear regression to get an idea of which features are important for a linear regressor.

After doing so, we see that the population weighted temp in the South has a pretty high feature importance. This may be due to the significant amount of refinery activity that occurs in the region (specifically the Gulf Coast). Alternatively, it may relate to vacationing patterns that occur in the South.

Weekly gas predictions
"""

target_features = ['Gas Prices']
explanatory_features = ['Crude Oil (Imports)', 'Total Petroleum Products (Imports)', 'Crude Oil and Petroleum Products (Imports)', 'Crude Oil (Exports)', 'Total Petroleum Products (Exports)', 'Crude Oil and Petroleum Products (Exports)', 'Crude Oil (Net Imports)', 'Total Petroleum Products (Net Imports)', 'Crude Oil and Petroleum Products (Net Imports)'] + [f'population_weighted_temp_{region}' for region in ['West', 'South', 'Northeast', 'Midwest']]
x_features = [feat + '_diff_weekly' for feat in explanatory_features] + [f'population_weighted_temp_{region}' for region in ['West', 'South', 'Northeast', 'Midwest']]

y = merged_df_simple['Gas Prices_diff_weekly'].copy()
X = merged_df_simple[x_features].copy()
X = X.fillna(0)
y = y.fillna(0)

scalar = StandardScaler()
X = scalar.fit_transform(X)
y = scalar.fit_transform(y.to_numpy().reshape(-1,1))

train_size = 0.7
validation_size = 0.15
test_size = 1 - train_size - validation_size

X_train = X[:int(train_size * len(X))]
y_train = y[:int(train_size * len(X))]

X_validation = X[int(train_size * len(X)) : int((train_size + validation_size) * len(X))]
y_validation = y[int(train_size * len(X)) : int((train_size + validation_size) * len(X))]

X_test = X[int((train_size + validation_size) * len(X)) : ]
y_test = y[int((train_size + validation_size) * len(X)) : ]

lin_model = LinearRegression()
lin_model.fit(X_train, y_train)
print("Training error for linear regression:", mean_squared_error(lin_model.predict(X_train), y_train))
print("Validation error for linear regression:", mean_squared_error(lin_model.predict(X_validation), y_validation))
print()

feature_importance = np.abs(lin_model.coef_)[0]
importance_df = pd.DataFrame({
    'Feature': x_features,
    'Importance': feature_importance
})

importance_df = importance_df.sort_values(by='Importance', ascending=False)
display(importance_df)


fig = go.Figure(data=go.Bar(y=importance_df['Feature'], x=importance_df['Importance'], orientation='h'))
fig.update_layout(
    title=f"Feature Importance for Predicting Weekly Change in Gas Price (Linear Regression)",
    yaxis_title="Feature",
    xaxis_title="Absolute Coefficient for Linear Regression",
    height=1000,
)
fig.show()

"""## Random Forest Regressor Feature Importance

We use the feature importances from a Random Forest Regressor to get an idea of which features are important for this model.

After doing so, we notice again that the population weighted temp in the South has a pretty high feature importance. The Northeast population weighted temp also seems quite important.
"""

model = RandomForestRegressor(max_depth=5)
model.fit(X_train, y_train.reshape(-1,))
print("Training error for random forest regression:", mean_squared_error(model.predict(X_train), y_train))
print("Validation error for random forest regression:", mean_squared_error(model.predict(X_validation), y_validation))
print()

feature_importance = np.abs(model.feature_importances_)
importance_df = pd.DataFrame({
    'Feature': x_features,
    'Importance': feature_importance
})

importance_df = importance_df.sort_values(by='Importance', ascending=False)
display(importance_df)


fig = go.Figure(data=go.Bar(y=importance_df['Feature'], x=importance_df['Importance'], orientation='h'))
fig.update_layout(
    title=f"Feature Importance for Predicting Weekly Change in Gas Prices (Random Forest Regressor)",
    yaxis_title="Feature",
    xaxis_title="Feature Importance for Random Forest Regressor",
    height=1000,
)
fig.show()

"""Both the Random Forest feature importance graph and the Linear Regression coefficient graph provide a comprehensive illustration of the importance of each feature in the dataset. In both approaches, the *population_weighted_temp*-related in the South, Northeast, and West regions, are the most influential factors."""

print(X_train.shape)
print(type(X_train))
print(type(x_features))
print(len(x_features))

"""## Recurrent Neural Network Feature Importance

We use permutation importance for the RNN to assess which features have the most significant impact on predictions. This method involves shuffling the values of each feature and measuring the resulting change in model performance. Features that cause a larger drop in performance are deemed more important.
"""

class RNN(nn.Module):
    def __init__(self, input_size, hidden_size=4, num_layers=2):
        super(RNN, self).__init__()
        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, x):
        batch_size = x.size(0)
        h0 = torch.zeros(self.rnn.num_layers, batch_size, self.rnn.hidden_size).to(x.device)
        out, _ = self.rnn(x, h0)
        out = out[:, -1, :]
        out = self.fc(out)
        return out

def create_sequences(data, targets, seq_length):
    sequences = []
    seq_targets = []
    for i in range(len(data) - seq_length):
        seq = data[i:i + seq_length]
        target = targets[i + seq_length]
        sequences.append(seq)
        seq_targets.append(target)
    return torch.stack(sequences), torch.tensor(seq_targets).reshape(-1, 1)


def compute_feature_importance(model, X, y, feature_names, metric=mean_squared_error):
    model.eval()
    baseline_preds = model(X).detach().cpu().numpy()
    baseline_score = metric(y.cpu().numpy(), baseline_preds)

    importances = []
    for feature_idx in range(X.shape[2]):
        X_permuted = X.clone()
        X_permuted[:, :, feature_idx] = X_permuted[:, torch.randperm(X.shape[1]), feature_idx]

        permuted_preds = model(X_permuted).detach().cpu().numpy()
        permuted_score = metric(y.cpu().numpy(), permuted_preds)

        importance = permuted_score - baseline_score
        importances.append((feature_names[feature_idx], importance))

    return importances

feature_names = ['Crude Oil (Imports)_diff_weekly', 'Total Petroleum Products (Imports)_diff_weekly', 'Crude Oil and Petroleum Products (Imports)_diff_weekly', 'Crude Oil (Exports)_diff_weekly', 'Total Petroleum Products (Exports)_diff_weekly', 'Crude Oil and Petroleum Products (Exports)_diff_weekly', 'Crude Oil (Net Imports)_diff_weekly', 'Total Petroleum Products (Net Imports)_diff_weekly', 'Crude Oil and Petroleum Products (Net Imports)_diff_weekly', 'population_weighted_temp_West_diff_weekly', 'population_weighted_temp_South_diff_weekly', 'population_weighted_temp_Northeast_diff_weekly', 'population_weighted_temp_Midwest_diff_weekly', 'population_weighted_temp_West', 'population_weighted_temp_South', 'population_weighted_temp_Northeast', 'population_weighted_temp_Midwest']

X_rnn_importance = torch.from_numpy(X_train).to(torch.float32)
y_rnn = torch.from_numpy(y_train).reshape(-1, 1).to(torch.float32)

#print(X_rnn_importance.shape)
#print(y_rnn.shape)

sequence_length = 10  # Set sequence window size
X_rnn_sequences, y_rnn_sequences = create_sequences(X_rnn_importance, y_rnn, sequence_length)

#print(f'X_train_sequences shape: {X_rnn_sequences.shape}')
#print(f'y_train_sequences shape: {y_rnn_sequences.shape}')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

input_size = X_rnn_sequences.shape[2]
model = RNN(input_size=input_size).to(device)

X_rnn_sequences = X_rnn_sequences.to(device)
y_rnn_sequences = y_rnn_sequences.to(device)

optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
criterion = nn.MSELoss()

for epoch in range(200):
    model.train()
    optimizer.zero_grad()
    outputs = model(X_rnn_sequences)
    loss = criterion(outputs, y_rnn_sequences)
    loss.backward()
    optimizer.step()
    #print(f"Epoch [{epoch + 1}/100], Loss: {loss.item():.4f}")

feature_importances = compute_feature_importance(model, X_rnn_sequences, y_rnn_sequences, feature_names)

sorted_importances = sorted(feature_importances, key=lambda x: abs(x[1]), reverse=True)

print("\nFeature Importances:")
for feature, importance in sorted_importances:
    print(f"{feature}: {importance:.4f}")

features = [feature for feature, _ in sorted_importances]
importances = [importance for _, importance in sorted_importances]

fig = go.Figure(data=go.Bar(y=features, x=importances, orientation='h'))
fig.update_layout(
    title="Feature Importance for Predicting Weekly Change in Gas Prices (RNN)",
    yaxis_title="Feature",
    xaxis_title="Feature Importance for RNN",
    height=1000,
)
fig.show()

"""After applying this method, we see that *population_weighted_temp_West_diff_weekly* has the highest feature importance. This suggests that temperature fluctuations in the western region play a pivotal role in influencing the target variable (gas price). The importance of this feature may stem from weather-driven demand variations in energy consumption, or perhaps specific industrial activities sensitive to temperature changes. Interestingly, *population_weighted_temp_Midwest_diff_weekly* and *Crude Oil (Imports)_diff_weekly* also exhibit high importance. Comparing to previous two methods utilizing linear regression and random forest, there are slight ordering difference; however, the population_weighted_temp group dominates the feature area in all approaches.

# Summary

In this notebook, we examined feature importance for prediction of gas price changes. We first conducted two generalized methods for evaluating feature importance: Linear Regression and Random Forest. Then, we focused on feature importance for our deep learning RNN model.

Linear Regression feature importance was done by comparing the absolute coefficients assigned to each feature by the regression model, and larger coefficients indicate a greater impact on the prediction/output. In this case, we found that temperatures, particularly in the South and Northeast, have a significant linear relationship with weekly changes in gas prices.

Random Forest feature importance was done through Gini importance by observing how much each feature contributes to node splits in the decision tree. Similar to linear regression, we found that temperatures, especially in the South and Northeast regions, play a key role in predicting changes in gas price.

Lastly, RNN feature importance was done by perturbing each feature independently in the test dataset and performing inference every time. By comparing the new prediction error with the baseline prediction error, we can gauge how much each feature impacts model performance and rank by the difference in error (larger difference in error implies greater impact). In this case, regional temperature also played a significant role, but more importantly, oil product imports also play a large factor.


All three methods highlight the importance of regional temperature features, particularly those associated with the South, Northeast, West, and Midwest. Additionally, weekly changes in crude oil imports also emerge as a significant factor, especially for the RNN model. These findings collectively indicate that temperatures and imports are key drivers of gas price fluctuations.
"""