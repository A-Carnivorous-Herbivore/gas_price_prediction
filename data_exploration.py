# -*- coding: utf-8 -*-
"""Data Exploration.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wOt26nPE9G9STfc98x9JH36QatBuR3at

# Initial Setup
"""

# !pip install netcdf4 pydap

import gdown
import numpy as np
import pandas as pd
import xarray as xr
import xarray as xr
import datetime as dt
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

exports_url = "https://drive.google.com/file/d/1J27j-yAZhe9tiQiJbctpL44QJ97nA4sV/view?usp=drive_link"
imports_url = "https://drive.google.com/file/d/1HS8YdIXwkHTxnq0CzJzJX1wUi_dvtznM/view?usp=drive_link"
net_imports_url = "https://drive.google.com/file/d/1ibxyKMBfs1WFNuYyUkmxf9k9jC_zuniX/view?usp=drive_link"
gas_prices_url = "https://drive.google.com/file/d/1rODElv0oLcJGLZufTfoLFhastfiMaLfX/view?usp=sharing"
tavg_url = "https://drive.google.com/file/d/1l6RLFWmVHtsQP4nGPKta9zvJAiJR71DS/view?usp=sharing"

exports = pd.read_csv('https://drive.google.com/uc?export=download&id=' + exports_url.split('/')[-2], skiprows=2)
imports = pd.read_csv('https://drive.google.com/uc?export=download&id=' + imports_url.split('/')[-2], skiprows=2)
net_imports = pd.read_csv('https://drive.google.com/uc?export=download&id=' + net_imports_url.split('/')[-2], skiprows=2)
gas_prices = pd.read_csv('https://drive.google.com/uc?export=download&id=' + gas_prices_url.split('/')[-2], skiprows=2)

"""Notes:
- **Exports**: Data (except for exports of crude oil and petroleum) is available after June 04, 2010. It is available for ethanol after May 26, 2023.
- **Imports**: Data for imports of crude oil, petroleum available after Jan 5, 1990. Summation of these only available after Feb 8, 1991.
- **Net imports**: This is simply imports - exports, but some of the data is unavailable (even when the data is available for both imports and exports). Thus, it is best to construct these features ourselves.
-  **Gas Prices**: Available April 5, 1993 - Oct 11, 2024. Will probably need to merge the rest of the data based on these dates.


General Notes:
- All dates are weekly
"""

exports = exports.drop(columns=['Unnamed: 11'])
exports['Date'] = pd.to_datetime(exports['Date'])
imports = imports.drop(columns=['Unnamed: 36'])
imports['Date'] = pd.to_datetime(imports['Date'])
gas_prices['Date'] = pd.to_datetime(gas_prices['Date'])

# Last value of each df is always NaN
exports = exports[:-1]
imports = imports[:-1]
gas_prices = gas_prices[:-1]

export_columns = {}
for column in exports.columns:
  export_columns[column] = ' '.join(column.replace('Weekly U.S.', '').replace('Exports of', '').replace('(Thousand Barrels per Day)', '(Exports)').split())

import_columns = {}
for column in imports.columns:
  import_columns[column] = ' '.join(column.replace('Weekly U.S.', '').replace('Imports of', '').replace('(Thousand Barrels per Day)', '(Imports)').split())

exports = exports.rename(columns = export_columns)
imports = imports.rename(columns = import_columns)
gas_prices = gas_prices.rename(columns = {'Weekly U.S. All Grades All Formulations Retail Gasoline Prices  (Dollars per Gallon)': 'Gas Prices'})

"""Note: For now, we can use `merged_df_simple`, which only has date, gas price, and imports/exports of crude oil and petroleum. In the future, we may look at more advanced features, which are contained in `merged_df_all_features`.

Merge data and find net imports (imports - exports)
"""

# Merge
merged_df_all_features = pd.merge_asof(gas_prices, imports, on='Date', direction='backward')
merged_df_all_features = pd.merge_asof(merged_df_all_features, exports, on='Date', direction='backward')

# Find Net Imports (Similar to net_imports df but with all dates)
reduced_columns = ['Crude Oil', 'Total Petroleum Products', 'Crude Oil and Petroleum Products']
for col in reduced_columns:
  merged_df_all_features[col + ' (Net Imports)'] = merged_df_all_features[col + ' (Imports)'] - merged_df_all_features[col + ' (Exports)']

cols = ['Date', 'Gas Prices'] + [col + ' (Imports)' for col in reduced_columns] + [col + ' (Exports)' for col in reduced_columns] + [col + ' (Net Imports)' for col in reduced_columns]

merged_df_simple = merged_df_all_features[cols]

"""Training, validation, testing (for use later)"""

# Get training, validation, testing sets
train_size = 0.7
validation_size = 0.15
n = len(merged_df_simple)

training_simple = merged_df_simple[:int(train_size * n)]
validation_simple = merged_df_simple[int(train_size * n):int((train_size + validation_size) * n)]
test_simple = merged_df_simple[int((train_size + validation_size) * n):]

"""# Data Analysis

See how gas changes in the past week/month. It's unsure which timeframe will be the best for prediction.
"""

merged_df_simple = merged_df_simple.assign(gas_prices_diff_weekly = merged_df_simple['Gas Prices'].diff())
merged_df_simple = merged_df_simple.assign(gas_prices_pct_weekly = merged_df_simple['Gas Prices'].pct_change())
merged_df_simple = merged_df_simple.assign(gas_prices_diff_monthly = merged_df_simple['Gas Prices'] - merged_df_simple['Gas Prices'].shift(4))
merged_df_simple = merged_df_simple.assign(gas_prices_pct_monthly = (merged_df_simple['Gas Prices'] - merged_df_simple['Gas Prices'].shift(4)) / (merged_df_simple['Gas Prices'].shift(4)))

"""Missing data:
- Only at the start for changes (i.e. diff/pct change)
- One missing one data point for exports (Fix with forward fill for simplicity)
"""

for col in merged_df_simple.columns:
  if len(merged_df_simple[merged_df_simple[col].isna()]) > 0:
    print(f"Missing data for {col}:", len(merged_df_simple[merged_df_simple[col].isna()]))

merged_df_simple['Total Petroleum Products (Exports)'] = merged_df_simple['Total Petroleum Products (Exports)'].ffill()
merged_df_simple['Total Petroleum Products (Net Imports)'] = merged_df_simple['Total Petroleum Products (Exports)'] - merged_df_simple['Total Petroleum Products (Imports)']

"""### Descriptive Statistics

Note: We may want to standardize this data due to high mean and std for imports/exports of oil. Alternatively, we might want to do some stanardization by month/year/etc for imports/exports due to the time-series nature of the data.
"""

merged_df_simple.describe()

"""### Visualizations

Time-series visualizations

Note: We will likely need to create diff/pct change features for imports/exports or do standardization to account for the time-series aspect.
- E.g. Total exports are steadily increasing over time. However, gas prices increase and decrease at a macro level.
"""

fig = make_subplots(rows=4, cols=1)

features = ['Gas Prices', 'Crude Oil and Petroleum Products (Imports)', 'Crude Oil and Petroleum Products (Exports)', 'Crude Oil and Petroleum Products (Net Imports)']

for i in range(len(features)):
  feature = features[i]
  fig.add_trace(go.Scatter(x=merged_df_simple['Date'], y=merged_df_simple[feature], name=feature), row=i+1, col=1)

fig.update_layout(height=1000)
fig.show()

target_features = ['gas_prices_diff_weekly', 'gas_prices_pct_weekly', 'gas_prices_diff_monthly', 'gas_prices_pct_monthly']
explanatory_features = ['Crude Oil (Imports)', 'Total Petroleum Products (Imports)', 'Crude Oil and Petroleum Products (Imports)', 'Crude Oil (Exports)', 'Total Petroleum Products (Exports)', 'Crude Oil and Petroleum Products (Exports)', 'Crude Oil (Net Imports)', 'Total Petroleum Products (Net Imports)', 'Crude Oil and Petroleum Products (Net Imports)']

"""# Weather Data

Note: Dataset has avg temp per month for various latitude/longitude points in the continental US. The dataset was initially extracted from here: https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.ncdc%3AC00332.

Initial intuition: More extreme temperatures may lead to higher gasoline prices

## Save the weather data (to_pickle)

Commented out because we only need to do this once
"""

# gdown.download('https://drive.google.com/uc?id=1l6RLFWmVHtsQP4nGPKta9zvJAiJR71DS', 'nclimgrid_tavg.nc', quiet=False)
# db = xr.open_dataset('nclimgrid_tavg.nc', engine="netcdf4")

# # Iterate over each year
# for i in range(32):
#   db_tavg = db.variables['tavg'][:,:,:]
#   db_time = db.variables['time']
#   db_lat = db.variables['lat']
#   db_long = db.variables['lon']

#   start_time = 1176 + i * 12
#   end_time = 1176 + (i + 1) * 12
#   year = 1993 + i
#   db_tavg = db_tavg[start_time:end_time]
#   db_time = db_time[start_time:end_time]

#   df = pd.DataFrame({'Time': [], 'Latitude': [], 'Longitude': [], 'Temp': []})

#   time = db_time.values
#   latitude = db_lat.values
#   longitude = db_long.values

#   time_grid, lat_grid, lon_grid = np.meshgrid(time, latitude, longitude, indexing='ij')

#   time_flat = time_grid.flatten()
#   lat_flat = lat_grid.flatten()
#   lon_flat = lon_grid.flatten()
#   temp_flat = db_tavg.values.flatten()

#   # Create a pandas DataFrame with columns for time, latitude, longitude, and temperature
#   df = pd.DataFrame({
#       'time': time_flat,
#       'latitude': lat_flat,
#       'longitude': lon_flat,
#       'temperature': temp_flat
#   })

#   # Time, lat, lon
#   # 1556, 596, 1385

#   # Drop values that are nan (not valid within the continental US)
#   df = df.dropna()
#   df.to_pickle(f'avg_temp_{year}.pkl')

"""## Get the weather data (read_pickle)

Right now, we split the data into chunks based on year. In the future, we will likely reduce the size of the dataset by focusing only on certain latitudes and longitudes (such as those of major US cities).
"""

# Get these from GDrive
links = 'https://drive.google.com/file/d/1NRsZEY_V4BLAO1dS3ATYAYarhgC1M2S8/view?usp=drive_link, https://drive.google.com/file/d/1--mxpbEOtO6KUms5QchCGn4Nr-esk3Pr/view?usp=drive_link, https://drive.google.com/file/d/1-9XVF6HXPduILEZsiUytnihHZYKYOL9P/view?usp=drive_link, https://drive.google.com/file/d/1--uAFc38dP3b65Mc6X048mJOF0Mida3G/view?usp=drive_link, https://drive.google.com/file/d/1-DXv1fqzAKhycgrtLXwnbfdMhS2bd6hJ/view?usp=drive_link, https://drive.google.com/file/d/1-296vjGCDPp79ECGrCXkGYrp-0Wsu0eF/view?usp=drive_link, https://drive.google.com/file/d/1-KdqjQI0sb8LTKtK1CJip5t2OF7JBrWQ/view?usp=drive_link, https://drive.google.com/file/d/1-C8U2qIW9EzrkUI7om6eniSxgt-SovMr/view?usp=drive_link, https://drive.google.com/file/d/1-Jzo9N_H5xEO-OpZDgIy2pic6kV76NV2/view?usp=drive_link, https://drive.google.com/file/d/1-LdeBU-d9gZloVUTKJmrK1SDS5GPDuFZ/view?usp=drive_link, https://drive.google.com/file/d/1-KFRDB9H8D6K_OfaEgeddiwzUuFlD47K/view?usp=drive_link, https://drive.google.com/file/d/1-QzwUl_2JHMgU-rOL8nO_VpQq-STeffV/view?usp=drive_link, https://drive.google.com/file/d/1-VwyQpYQ6pNlRLcuJOUguggKTcEEl1q2/view?usp=drive_link, https://drive.google.com/file/d/1-Xm52Ygs28TFUDoSfm1CwtGfEQDrxr0i/view?usp=drive_link, https://drive.google.com/file/d/1-W7zxGX8TxQpUX06ZsgnWPYMsYya0xHx/view?usp=drive_link, https://drive.google.com/file/d/1-SWCb_Hd_cP4r9iqIGM31R9wFBBumx_q/view?usp=drive_link, https://drive.google.com/file/d/1-ZuW9QDzfklwENSv3jYB0X9xJEVfMIiO/view?usp=drive_link, https://drive.google.com/file/d/1-_GcHm7BDaCiggos4T12B0NoguA55sUe/view?usp=drive_link, https://drive.google.com/file/d/1-f6hE3cSqZf54Ac414ZafQJgUNDFApsB/view?usp=drive_link, https://drive.google.com/file/d/1-mIMLs48YiSHORu4gjXXI9OVSyxCmhwB/view?usp=drive_link, https://drive.google.com/file/d/1-n6KQagxJJLA38J11uV9vH0nZ3AF5NvW/view?usp=drive_link, https://drive.google.com/file/d/1-oPUOPwhgiMhHJt_2677BfeI0KdCoeRv/view?usp=drive_link, https://drive.google.com/file/d/1-p7issdOZv4bK-5X1nyTr8hQ5bCy8wIE/view?usp=drive_link, https://drive.google.com/file/d/1-rjrY-dS5Co4zrDeX5CiNUkZ1f6nmm5h/view?usp=drive_link, https://drive.google.com/file/d/1-v1l8IbKsMWFa1M73Vr3WNlR_ye_oOKD/view?usp=drive_link, https://drive.google.com/file/d/10-wLcAOkNKECLXBH_vQfs6eHh-2GQ0S8/view?usp=drive_link, https://drive.google.com/file/d/106ZCkzZdDAJocLgvhezanC9anLDralOP/view?usp=drive_link, https://drive.google.com/file/d/10EOXMGE95MHwe0hTk5YVWdBGmANYX7GP/view?usp=drive_link, https://drive.google.com/file/d/108hBksVRFdWoSWDSK9uAabqmjFnZNxTE/view?usp=drive_link, https://drive.google.com/file/d/10B8u88PHZKR9HJQEUOsm4AScXUduBSM4/view?usp=drive_link, https://drive.google.com/file/d/10Cwe4rf15xDQ9GATM7X4IbLZbI3oX8O-/view?usp=drive_link, https://drive.google.com/file/d/10Dt1XwpcdM45S4um8mm75VrZSbIf3AJJ/view?usp=drive_link'
year_id_map = {}
links = links.split(',')
for i in range(len(links)):
  year_id_map[1993 + i] = links[i].split('/')[-2]

def get_tavg_year_df(year):
  file_id = year_id_map[year]
  url = f'https://drive.google.com/uc?id={file_id}'
  output = f'avg_temp_{year}.pkl'
  gdown.download(url, output, quiet=True)
  return pd.read_pickle(output)

tavg_1993_df = get_tavg_year_df(1993)
tavg_1993_df.head()

"""## Baseline Model

At this stage, we will be using a rough baseline model to estimate the result range of future model. We will be using linear regression model for the estimation.
"""