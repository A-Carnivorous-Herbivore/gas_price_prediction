# -*- coding: utf-8 -*-
"""Metrics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HPXq-0fYS_DQkHzyc4oly6Hcyb1XPS5T

There is a summary at the bottom of this notebook that summarizes the work done in this notebook for this milestone! (Comments are included with code cells along the way as well.)

## Initial Setup
"""

# !pip install netcdf4 pydap

import torch
import gdown
import random
import numpy as np
import pandas as pd
import xarray as xr
import xarray as xr
import datetime as dt
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

"""## Setup Data

*Notes for this can be found in the Data Exploration notebook.*
"""

# set seed value for reproducibility
def set_seed(seed):
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    np.random.seed(seed)
    random.seed(seed)
seed = 42
set_seed(seed)

exports_url = "https://drive.google.com/file/d/1J27j-yAZhe9tiQiJbctpL44QJ97nA4sV/view?usp=drive_link"
imports_url = "https://drive.google.com/file/d/1HS8YdIXwkHTxnq0CzJzJX1wUi_dvtznM/view?usp=drive_link"
net_imports_url = "https://drive.google.com/file/d/1ibxyKMBfs1WFNuYyUkmxf9k9jC_zuniX/view?usp=drive_link"
gas_prices_url = "https://drive.google.com/file/d/1rODElv0oLcJGLZufTfoLFhastfiMaLfX/view?usp=sharing"
tavg_url = "https://drive.google.com/file/d/1l6RLFWmVHtsQP4nGPKta9zvJAiJR71DS/view?usp=sharing"
tavg_region_url = "https://drive.google.com/file/d/1y-XvXUpjxxHyl5q3aOpzZvyWWumFfk1r/view?usp=drive_link"

exports = pd.read_csv('https://drive.google.com/uc?export=download&id=' + exports_url.split('/')[-2], skiprows=2)
imports = pd.read_csv('https://drive.google.com/uc?export=download&id=' + imports_url.split('/')[-2], skiprows=2)
net_imports = pd.read_csv('https://drive.google.com/uc?export=download&id=' + net_imports_url.split('/')[-2], skiprows=2)
gas_prices = pd.read_csv('https://drive.google.com/uc?export=download&id=' + gas_prices_url.split('/')[-2], skiprows=2)
temps = pd.read_pickle('https://drive.google.com/uc?export=download&id=' + tavg_region_url.split('/')[-2])

exports = exports.drop(columns=['Unnamed: 11'])
exports['Date'] = pd.to_datetime(exports['Date'])
imports = imports.drop(columns=['Unnamed: 36'])
imports['Date'] = pd.to_datetime(imports['Date'])
gas_prices['Date'] = pd.to_datetime(gas_prices['Date'])

# Last value of each df is always NaN
exports = exports[:-1]
imports = imports[:-1]
gas_prices = gas_prices[:-1]

export_columns = {}
for column in exports.columns:
  export_columns[column] = ' '.join(column.replace('Weekly U.S.', '').replace('Exports of', '').replace('(Thousand Barrels per Day)', '(Exports)').split())

import_columns = {}
for column in imports.columns:
  import_columns[column] = ' '.join(column.replace('Weekly U.S.', '').replace('Imports of', '').replace('(Thousand Barrels per Day)', '(Imports)').split())

exports = exports.rename(columns = export_columns)
imports = imports.rename(columns = import_columns)
gas_prices = gas_prices.rename(columns = {'Weekly U.S. All Grades All Formulations Retail Gasoline Prices  (Dollars per Gallon)': 'Gas Prices'})

west = temps[temps['region'] == 'West']
midwest = temps[temps['region'] == 'Midwest']
south = temps[temps['region'] == 'South']
northeast = temps[temps['region'] == 'Northeast']
all_regions = [west, midwest, south, northeast]

# Merge
merged_df_all_features = pd.merge_asof(gas_prices, imports, on='Date', direction='backward')
merged_df_all_features = pd.merge_asof(merged_df_all_features, exports, on='Date', direction='backward')

for region in all_regions:
  curr_region = region.iloc[0]['region']
  region = region.add_suffix(f'_{curr_region}')
  merged_df_all_features = pd.merge_asof(merged_df_all_features, region[[f'time_{curr_region}', f'population_weighted_temp_{curr_region}']], direction='backward', left_on='Date', right_on=f'time_{curr_region}')
  merged_df_all_features = merged_df_all_features.drop(columns=[f'time_{curr_region}'])

# Find Net Imports (Similar to net_imports df but with all dates)
reduced_columns = ['Crude Oil', 'Total Petroleum Products', 'Crude Oil and Petroleum Products']
for col in reduced_columns:
  merged_df_all_features[col + ' (Net Imports)'] = merged_df_all_features[col + ' (Imports)'] - merged_df_all_features[col + ' (Exports)']

cols = ['Date', 'Gas Prices'] + [col + ' (Imports)' for col in reduced_columns] + [col + ' (Exports)' for col in reduced_columns] + [col + ' (Net Imports)' for col in reduced_columns] + [f'population_weighted_temp_{region}' for region in ['West', 'Northeast', 'South', 'Midwest']]

merged_df_simple = merged_df_all_features[cols].copy()

for col in merged_df_simple.columns:
  if len(merged_df_simple[merged_df_simple[col].isna()]) > 0:
    print(f"Missing data for {col}:", len(merged_df_simple[merged_df_simple[col].isna()]))

merged_df_simple.loc[:, 'Total Petroleum Products (Exports)'] = merged_df_simple['Total Petroleum Products (Exports)'].ffill()
merged_df_simple.loc[:, 'Total Petroleum Products (Net Imports)'] = merged_df_simple['Total Petroleum Products (Exports)'] - merged_df_simple['Total Petroleum Products (Imports)']

for col in merged_df_simple.columns:
  if col not in ['Date']:
    merged_df_simple[f'{col}_diff_weekly'] = merged_df_simple[col].diff()
    merged_df_simple[f'{col}_diff_monthly'] = merged_df_simple[col] - merged_df_simple[col].shift(4)

"""# Feature Importance"""

import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error
from sklearn.preprocessing import StandardScaler
import plotly.graph_objects as go
import plotly.express as px

"""## Linear Regression Feature Importance

We use the absolute coefficient for linear regression to get an idea of which features are important for a linear regressor.

After doing so, we see that the population weighted temp in the South has a pretty high feature importance. This may be due to the significant amount of refinery activity that occurs in the region (specifically the Gulf Coast). Alternatively, it may relate to vacationing patterns that occur in the South.

Weekly gas predictions
"""

target_features = ['Gas Prices']
explanatory_features = ['Crude Oil (Imports)', 'Total Petroleum Products (Imports)', 'Crude Oil and Petroleum Products (Imports)', 'Crude Oil (Exports)', 'Total Petroleum Products (Exports)', 'Crude Oil and Petroleum Products (Exports)', 'Crude Oil (Net Imports)', 'Total Petroleum Products (Net Imports)', 'Crude Oil and Petroleum Products (Net Imports)'] + [f'population_weighted_temp_{region}' for region in ['West', 'South', 'Northeast', 'Midwest']]
x_features = [feat + '_diff_weekly' for feat in explanatory_features] + [f'population_weighted_temp_{region}' for region in ['West', 'South', 'Northeast', 'Midwest']]

y = merged_df_simple['Gas Prices_diff_weekly'].copy()
X = merged_df_simple[x_features].copy()
X = X.fillna(0)
y = y.fillna(0)

scalar = StandardScaler()
X = scalar.fit_transform(X)
y = scalar.fit_transform(y.to_numpy().reshape(-1,1))

train_size = 0.7
validation_size = 0.15
test_size = 1 - train_size - validation_size

X_train = X[:int(train_size * len(X))]
y_train = y[:int(train_size * len(X))]

X_validation = X[int(train_size * len(X)) : int((train_size + validation_size) * len(X))]
y_validation = y[int(train_size * len(X)) : int((train_size + validation_size) * len(X))]

X_test = X[int((train_size + validation_size) * len(X)) : ]
y_test = y[int((train_size + validation_size) * len(X)) : ]

lin_model = LinearRegression()
lin_model.fit(X_train, y_train)
for metric in [mean_squared_error, mean_absolute_error, mean_absolute_percentage_error]:
  print(f"Training {metric.__name__}:", metric(y_train, lin_model.predict(X_train)))
  print(f"Validation {metric.__name__}:", metric(y_validation, lin_model.predict(X_validation)))
  print(f"Testing {metric.__name__}:", metric(y_test, lin_model.predict(X_test)))
  print()

# feature_importance = np.abs(lin_model.coef_)[0]
# importance_df = pd.DataFrame({
#     'Feature': x_features,
#     'Importance': feature_importance
# })

# importance_df = importance_df.sort_values(by='Importance', ascending=False)
# display(importance_df)


# fig = go.Figure(data=go.Bar(y=importance_df['Feature'], x=importance_df['Importance'], orientation='h'))
# fig.update_layout(
#     title=f"Feature Importance for Predicting Weekly Change in Gas Price (Linear Regression)",
#     yaxis_title="Feature",
#     xaxis_title="Absolute Coefficient for Linear Regression",
#     height=1000,
# )
# fig.show()

"""## Random Forest Regressor Feature Importance

We use the feature importances from a Random Forest Regressor to get an idea of which features are important for this model.

After doing so, we notice again that the population weighted temp in the South has a pretty high feature importance. The Northeast population weighted temp also seems quite important.
"""

model = RandomForestRegressor(max_depth=5)
model.fit(X_train, y_train.reshape(-1,))
for metric in [mean_squared_error, mean_absolute_error, mean_absolute_percentage_error]:
  print(f"Training {metric.__name__}:", metric(y_train, model.predict(X_train)))
  print(f"Validation {metric.__name__}:", metric(y_validation, model.predict(X_validation)))
  print(f"Testing {metric.__name__}:", metric(y_test, model.predict(X_test)))
  print()

# feature_importance = np.abs(model.feature_importances_)
# importance_df = pd.DataFrame({
#     'Feature': x_features,
#     'Importance': feature_importance
# })

# importance_df = importance_df.sort_values(by='Importance', ascending=False)
# display(importance_df)


# fig = go.Figure(data=go.Bar(y=importance_df['Feature'], x=importance_df['Importance'], orientation='h'))
# fig.update_layout(
#     title=f"Feature Importance for Predicting Weekly Change in Gas Prices (Random Forest Regressor)",
#     yaxis_title="Feature",
#     xaxis_title="Feature Importance for Random Forest Regressor",
#     height=1000,
# )
# fig.show()

"""Both the Random Forest feature importance graph and the Linear Regression coefficient graph provide a comprehensive illustration of the importance of each feature in the dataset. In both approaches, the *population_weighted_temp*-related in the South, Northeast, and West regions, are the most influential factors."""

print(X_train.shape)
print(type(X_train))
print(type(x_features))
print(len(x_features))

"""## Recurrent Neural Network Feature Importance

We use permutation importance for the RNN to assess which features have the most significant impact on predictions. This method involves shuffling the values of each feature and measuring the resulting change in model performance. Features that cause a larger drop in performance are deemed more important.
"""

class RNN(nn.Module):
    def __init__(self, input_size, hidden_size=4, num_layers=2):
        super(RNN, self).__init__()
        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, x):
        batch_size = x.size(0)
        h0 = torch.zeros(self.rnn.num_layers, batch_size, self.rnn.hidden_size).to(x.device)
        out, _ = self.rnn(x, h0)
        out = out[:, -1, :]
        out = self.fc(out)
        return out

def create_sequences(data, targets, seq_length):
    sequences = []
    seq_targets = []
    for i in range(len(data) - seq_length):
        seq = data[i:i + seq_length]
        target = targets[i + seq_length]
        sequences.append(seq)
        seq_targets.append(target)
    return torch.stack(sequences), torch.tensor(seq_targets).reshape(-1, 1)


def compute_feature_importance(model, X, y, feature_names, metric=mean_squared_error):
    model.eval()
    baseline_preds = model(X).detach().cpu().numpy()
    baseline_score = metric(y.cpu().numpy(), baseline_preds)

    importances = []
    for feature_idx in range(X.shape[2]):
        X_permuted = X.clone()
        X_permuted[:, :, feature_idx] = X_permuted[:, torch.randperm(X.shape[1]), feature_idx]

        permuted_preds = model(X_permuted).detach().cpu().numpy()
        permuted_score = metric(y.cpu().numpy(), permuted_preds)

        importance = permuted_score - baseline_score
        importances.append((feature_names[feature_idx], importance))

    return importances


# feature_names = ['Crude Oil (Imports)_diff_weekly', 'Total Petroleum Products (Imports)_diff_weekly', 'Crude Oil and Petroleum Products (Imports)_diff_weekly', 'Crude Oil (Exports)_diff_weekly', 'Total Petroleum Products (Exports)_diff_weekly', 'Crude Oil and Petroleum Products (Exports)_diff_weekly', 'Crude Oil (Net Imports)_diff_weekly', 'Total Petroleum Products (Net Imports)_diff_weekly', 'Crude Oil and Petroleum Products (Net Imports)_diff_weekly', 'population_weighted_temp_West_diff_weekly', 'population_weighted_temp_South_diff_weekly', 'population_weighted_temp_Northeast_diff_weekly', 'population_weighted_temp_Midwest_diff_weekly', 'population_weighted_temp_West', 'population_weighted_temp_South', 'population_weighted_temp_Northeast', 'population_weighted_temp_Midwest']


X_train_torch = torch.from_numpy(X_train).to(torch.float32)
X_validation_torch = torch.from_numpy(X_validation).to(torch.float32)
X_test_torch = torch.from_numpy(X_test).to(torch.float32)

y_train_torch = torch.from_numpy(y_train).reshape(-1,1).to(torch.float32)
y_validation_torch = torch.from_numpy(y_validation).reshape(-1,1).to(torch.float32)
y_test_torch = torch.from_numpy(y_test).reshape(-1,1).to(torch.float32)


sequence_length = 10 # Set window size
X_train_sequences, y_train_sequences = create_sequences(X_train_torch, y_train_torch, sequence_length)
X_validation_sequences, y_validation_sequences = create_sequences(X_validation_torch, y_validation_torch, sequence_length)
X_test_sequences, y_test_sequences = create_sequences(X_test_torch, y_test_torch, sequence_length)


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

input_size = X_train_sequences.shape[2]
model = RNN(input_size=input_size).to(device)

X_rnn_sequences = X_train_sequences.to(device)
y_rnn_sequences = y_train_sequences.to(device)

optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
criterion = nn.MSELoss()

for epoch in range(200):
    model.train()
    optimizer.zero_grad()
    outputs = model(X_rnn_sequences)
    loss = criterion(outputs, y_rnn_sequences)
    loss.backward()
    optimizer.step()
    #print(f"Epoch [{epoch + 1}/100], Loss: {loss.item():.4f}")

model.eval()
with torch.no_grad():
    train_outputs = model(X_train_sequences)
    validation_outputs = model(X_validation_sequences)
    test_outputs = model(X_test_sequences)

    for metric in [mean_squared_error, mean_absolute_error, mean_absolute_percentage_error]:
      print(f"Training {metric.__name__}:", metric(y_train_sequences, train_outputs))
      print(f"Validation {metric.__name__}:", metric(y_validation_sequences, validation_outputs))
      print(f"Test {metric.__name__}:", metric(y_test_sequences, test_outputs))
      print()



target_features = ['Gas Prices']
explanatory_features = ['Crude Oil (Imports)', 'Total Petroleum Products (Imports)', 'Crude Oil and Petroleum Products (Imports)', 'Crude Oil (Exports)', 'Total Petroleum Products (Exports)', 'Crude Oil and Petroleum Products (Exports)', 'Crude Oil (Net Imports)', 'Total Petroleum Products (Net Imports)', 'Crude Oil and Petroleum Products (Net Imports)'] + [f'population_weighted_temp_{region}' for region in ['West', 'South', 'Northeast', 'Midwest']]
x_features = [feat + '_diff_weekly' for feat in explanatory_features] + [f'population_weighted_temp_{region}' for region in ['West', 'South', 'Northeast', 'Midwest']]

y = merged_df_simple['Gas Prices_diff_weekly'].copy()
X = merged_df_simple[x_features].copy()
X = X.fillna(0)
y = y.fillna(0)

import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

fig = make_subplots(rows=1, cols=1)
fig.add_trace(go.Scatter(x=merged_df_simple['population_weighted_temp_South'], y=merged_df_simple['Gas Prices'], name='Gas Prices', mode='markers'), row=1, col=1)
fig.show()
print()
print(merged_df_simple['population_weighted_temp_South'].corr(merged_df_simple['Gas Prices']))

merged_df_simple.corr()['Gas Prices']

merged_df_simple[merged_df_simple['Date'].isin([dt.datetime(2023, 1, 2), dt.datetime(2023, 4, 3), dt.datetime(2023, 7, 3), dt.datetime(2023, 10, 2)])][['Date', 'Gas Prices', 'Crude Oil (Net Imports)', 'Total Petroleum Products (Net Imports)', 'population_weighted_temp_South', 'population_weighted_temp_Northeast', 'population_weighted_temp_West', 'population_weighted_temp_Midwest']]

for col in merged_df_simple.columns:
  print(col)

fig = make_subplots(rows=2, cols=1)
reduced = merged_df_simple[(merged_df_simple['Date'] <= dt.datetime(2024, 1, 1)) & (merged_df_simple['Date'] >= dt.datetime(2018, 1, 1))]

fig.add_trace(go.Scatter(x=reduced['Date'], y=reduced['Gas Prices'], name='Gas Prices', line=dict(width=4)), row=1, col=1)
fig.add_trace(go.Scatter(x=reduced['Date'], y=reduced['population_weighted_temp_South'], name='Avg Temp (South)', line=dict(width=4)), row=2, col=1)

